Thinking1	在CTR点击率预估中，使用GBDT+LR的原理是什么？能简要说明GBDT和LR在CTR预估中的作用（10point）


Thinking2	Wide & Deep的模型结构是怎样的，为什么能通过具备记忆和泛化能力（memorization and generalization）1、能简要说明Wide&Deep的模型（5point）
2、如何具备记忆和泛化能力（5point）



Thinking3	在CTR预估中，使用FM与DNN结合的方式，有哪些结合的方式，代表模型有哪些？1、能说出哪两种FM和DNN的组合方式（5points）
2、能说出FM和DNN的组合出的算法（5points）


Thinking4	GBDT和随机森林都是基于树的算法，它们有什么区别？能简要说明这两种基于树的算法的不同（10points）


Thinking5	item流行度在推荐系统中有怎样的应用1、冷启动中的使用（10points)
2、协同过滤中的TopN推荐（10points）
3、其他使用（+5points)


Thinking1	答：在CTR预估问题中，样本数量大，点击率预估模型中的训练样本可达上亿级别，以往的CTR预估采用LR模型，LR是线性模型，虽然速度较快，但是学习能力有限，而且人工成本高，为了更好的进行特征提取，提升LR的学习能力，需要采用人工特征工程，即通过人工方式找到有区分度的特征、特征组合。对人的要求高，时间成本高；
GBDT+LR算法通过GBDT将特征进行组合，然后传入给线性分类器，然后LR对GBDT产生的输入数据进行分类。GBDT进行新特征构造，把模型中的每棵树计算得到的预测概率值所属的叶子结点位置记为1，构造新的训练数据。保留大部分的重要特征，大量弱特征的累积也很重要，不能都去掉。如果去掉部分不重要的特征，对模型的影响比较小。GBDT是一种常用的非线性模型，基于集成学习中的boosting思想，也就是每次迭代都在减少残差的梯度方向新建立一颗决策树，迭代多少次就会生成多少颗决策树。GBDT得到的特征、特征组合都具有区分性，在特征选择上不亚于人工经验的处理方式。使用GBDT+LR，相比单纯的LR和GBDT，在Loss上减少了3%，提升作用明显；同等条件下，ROC曲线面积也优于LR、RF、LR+RF、GBDT。综上GBDT+LR在CTR中能够有很好的效果。


Thinking2	答：分成两部分：记忆能力、泛化能力，然后再将二者组合
记忆能力（memorization）：	记忆能力，学习items或者features之间的相关频率，在历史数据中探索相关性的可行性。Wide推荐：系统通过获得用户的购物日志数据，包括用户点击哪些商品，购买过哪些商品，然后通过OneHot编码转换为离散特征。好处是可解释性强，不足在于特征组合需要人为操作。采用Linear Regression，特征组合需要人来设计，一个样本有d个特征X1-XN，然后构建线性模型，得到其每个特征的权重，实际过程中还有可能存在特征交叉，得到一个Wide模型(公式见附图)。(wide模型是线性模型，输入特征可以是连续特征，也可以是稀疏的离散特征。离散特征通过交叉可以组成更高维的离散特征。)

泛化能力（generalization）：泛化（推理）能力，基于相关性的传递，去探索一些在过去没有出现过的特征组合。Deep推荐：通过深度学习出一些向量，这些向量是隐性特征，往往没有可解释性的。使用的特征：连续特征，Embedding后的离散特征，使用前馈网络模型，特征首先转换为低维稠密向量，作为第一个隐藏层的输入，解决维度爆炸问题根据最终的loss反向训练更新。向量进行随机初始化，隐藏层的激活函数通常使用ReLU。这样训练出来的模型就具有了很强的泛化能力，即使来了一个新的向量，也能够很好的适应。

二者组合：采用ensemble的方法，两个模型分别对全量数据进行预测，然后根据权重组合最终的预测结果。joint training：wide和deep的特征合一，构成一个模型进行预测。


Thinking3	答：比如FNN算法使用FM进行参数初始化，用deep模型进行训练；比如deepFM模型，将thiking2中的wide部分换成了FM；比如NFM算法，串行架构，将FM的结果作为DNN的输入
。是对embedding直接采用对位相乘（element-wise）后相加起来作为交叉特征，然后通过DNN直接将特征压缩，最后concatenate linear部分和deep部分的特征。


Thinking4	答：1、随机森林采用的bagging思想，而GBDT采用的boosting思想。这两种方法都是Bootstrap思想的应用，Bootstrap是一种有放回的抽样方法思想。虽然都是有放回的抽样，但二者的区别在于：Bagging采用有放回的均匀取样，而Boosting根据错误率来取样（Boosting初始化时对每一个训练样例赋相等的权重1／n，然后用该算法对训练集训练t轮，每次训练后，对训练失败的样例赋以较大的权重），因此Boosting的分类精度要优于Bagging。Bagging的训练集的选择是随机的，各训练集之间相互独立，弱分类器可并行，而Boosting的训练集的选择与前一轮的学习结果有关，是串行的。
2、组成随机森林的树可以是分类树，也可以是回归树；而GBDT只能由回归树组成。
3、组成随机森林的树可以并行生成；而GBDT只能是串行生成。
4、对于最终的输出结果而言，随机森林采用多数投票等；而GBDT则是将所有结果累加起来，或者加权累加起来。
5、随机森林对异常值不敏感；GBDT对异常值非常敏感。
6、随机森林对训练集一视同仁；GBDT是基于权值的弱分类器的集成。
7、随机森林是通过减少模型方差提高性能；GBDT是通过减少模型偏差提高性能。


Thinking5	答：冷启动：新用户在冷启动阶段更倾向于热门排行榜的，什么内容流行度越高，就给用户推荐什么。本质上就是什么内容对用户吸引力高，就给用户推荐什么。
协同过滤中的TopN推荐：协同过滤的原理是挖掘相似的用户，通过他们的行为进行推荐，但是有可能会存在虚假恶意的行为，故意增加或者压制某些item被推荐的可能性，比如一些水军利用协同过滤给某个商品作弊，但是他这个作弊的商品不在这个网站的流行度商品列表里面，所以他作弊的商品不会推荐给用户，起到一个反作弊的作用。

			
	