Thinking1	机器学习中的监督学习、非监督学习、强化学习有何区别
		简要说出三者之间的区别（10points）	
		答：监督学习：监督学习已经准备好的训练数据输出值
		    非监督学习：非监督学习中既没有输出值也没有奖励值的，只有数据特征；
		    非监督学习与监督学习一样，数据之间也都是独立的，没有强化学习这样的前后依赖关系。
		    强化学习：强化学习只有奖励值（为负是为惩罚），但是这个奖励值和监督学习的输出值不一样，它不是事先给出的，而是延后给出的（比如走路摔倒）
				

Thinking2	什么是策略网络，价值网络，有何区别
		简要说明两者之间的区别（10points）
		答：策略网络就是，对于给定的输入，通过学习给出一个确定输出的网络：（动作1，状态1），（动作2，状态2）
		    价值网络：计算目前状态s的累积分数的期望，价值网络给游戏中的状态赋予一个分数（数值），每个状态都经历了整个数值网络
		    区别：策略网络的输出，是一个落子的概率分布；价值网络的输出，一个可能获胜的数值，即“价值”，这个价值训练是一种回归				    (regression)，即调整网络的权重来逼近每一种棋局真实的输赢预测。	

Thinking3	请简述MCTS（蒙特卡洛树搜索）的原理，4个步骤Select, Expansion，Simluation，Backpropagation是如何操作的
		简要说明4个步骤的原理（10points）	
		答：选择，从根节点开始，按一定策略，搜索到叶子节点
		    扩展，对叶子节点扩展一个或多个合法的子节点
                    模拟，对子节点采用随机的方式（这也是为什么称之为蒙特卡洛的原因）模拟若干次实验。模拟到最终状态时即可得到当前模拟所得的分数
		    回传，根据子节点若干次模拟的得分，更新当前子节点的模拟次数与得分值。同时将模拟次数与得分值回传到其所有祖先节点并更新祖先节点	

Thinking4：	假设你是抖音的技术负责人，强化学习在信息流推荐中会有怎样的作用，如果要进行使用强化学习，都有哪些要素需要考虑							简要说明强化学习在信息流推荐中的应用，有自己的观点，（10points）	
		答：根据用户对视频的浏览时间来决定确定用户是否喜欢这个、之类的视频，如果用户投诉举报了这类视频，那么下次不再为这个用户推荐这种类型的视频
		如果用户反复多次观看了这个视频，那么以后多给这个用户推荐这个视频。除此之外，一定要加上一定较大比例的随机推荐，因为老是给用户推荐那几种视频
		用户会厌烦的。

Thinking5：	在自动驾驶中，如何使用强化学习进行训练，请说明简要的思路
		简要说明强化学习在自动驾驶中的应用，有自己的观点，（10points）	
		答：因为强化学习模型需要成千上万次的试错来迭代训练，而真实车辆在路面上很难承受如此多的试错；模拟器渲染的虚拟图像转换为真实图像，并用合成的
                真实图像训练强化学习代理。然后根据仿真的运动结果来训练驾驶的策略。运用强化学习的思想，经过大量训练之后，可以在真实的路上跑。