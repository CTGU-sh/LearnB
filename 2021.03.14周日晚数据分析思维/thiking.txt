Thinking1	新零售中的“人、货、场”分别指的是什么？
		能简要说明人货场的概念（10points）
Thinking1	答：人指的是客户，对客户画像（包括性别、年龄、兴趣、是否单身）等分析，
		比如利用对人群分析，对比促销前后的增长情况，重点关注增长明星的群体；
		    货指的是产品的定价、搭配（爆款、形象、搭配产品）、仓库的配送和供应链管理
		分析产品数据（数据、点击、订单、入篮量），帮助企业了解不同商品的用户关注度、购买力，
		为产品生命周期分析、产品推广策略提供数据支持；
		    场指的是消费的场所、场景，对各个场景（线上、线下）优化，然后更好的跟用户进行服务与交流。    

Thinking2	AIPL与传统的品牌资产评估有何区别？
		能简要说明AIPL区别传统的品牌资产评估方法的区别（10points）
		答：A(Awareness)认知  I(Interest)兴趣 P(Puchase)购买 L(Loyalty)忠诚
		    把用户分成4部分，然后分析4部分中的性别、人均月消费、年龄差差异、人生阶段差异分别对应不同类别的用户
		    做出改变，进而保证品牌的健壮性。

Thinking3	请列举一例生活工作中存在的帕累托法则			
		能简要说明帕累托法则的例子（10points）
		答：比如世界中的财富分配，服从于二八原则；再比如git、Python一些基本的语法，基本上常用的命令（20%的函数）		
		能够完成80%的任务。
	
Thinking4	请简述GBDT与XGBoost的区别？
		简要说出GBDT与XGBoost的区别（20points）
		答：传统GBDT以CART作为基分类器，xgboost还支持线性分类器；
		    传统GBDT在优化时只用到一阶导数信息，xgboost则对代价函数进行了二阶泰勒展开，同时用到了一阶和二阶导数；
		    Shrinkage（缩减），相当于学习速率（xgboost中的eta）。xgboost在进行完一次迭代后，会将叶子节点的权重乘
		    上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间。实际应用中，一般把eta设置得小一点，然后
                    迭代次数设置得大一点。（传统GBDT的实现也有学习速率）；
                    列抽样（column subsampling）。xgboost借鉴了随机森林的做法，支持列抽样，不仅能降低过拟合，还能减少计算，
		    这也是xgboost异于传统gbdt的一个特性。对缺失值的处理。对于特征的值有缺失的样本，xgboost可以自动学习出它的分裂方向；
		    xgboost工具支持并行，xgboost的并行不是tree粒度的并行，xgboost也是一次迭代完才能进行下一次迭代的（第t次迭代的代价函
		    数里包含了前面t-1次迭代的预测值）。xgboost的并行是在特征粒度上的，预先对数据进行了排序，然后保存为block结构，后面的
		    迭代中重复地使用这个结构，大大减小计算量，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，
		    那么各个特征的增益计算就可以开多线程进行。（GBDT主要是串行）

Thinking5	如何处理神经网络中的过拟合问题？
		简要说明处理神经网络过拟合问题的方法（20points）
		答：可能是数据不纯导致的，如果出现了过拟合就需要我们重新清洗数据；
		    增大数据的训练量，用于训练的数据量太小导致的，训练数据占总数据的比例过小；
		    采用正则化，降低高次方的权重；
		    采用dropout方法，就是dropout方法在训练的时候让神经元以一定的概率不工作；
		     等等。
	
